\documentclass{article}

% AMS math symbols
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

% colored textare that buyers
\usepackage{color}

% Evolutional algorithms journal template
\usepackage{ecj}

% custom enumerates
\usepackage{enumitem}

% eps figures
\usepackage[pdftex]{graphicx}

% boxed figures
\usepackage{float}

% set page margins
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

% citations with author
\usepackage{natbib}

% additional colors
\usepackage[usenames,dvipsnames]{xcolor}

\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}
\newcommand{\usman}{\textcolor{Red}{\textbf{*** USMAN ***} }}
\newcommand{\claudio}{\textcolor{Cerulean}{\textbf{*** CLAUDIO ***} }}

%\floatstyle{boxed} 
%\restylefloat{figure}

\setlength{\parskip}{7pt}
\setlength{\parsep}{0pt}
\setlength{\parindent}{0pt}

\setlist{nolistsep}

% ADDING A FIGURE
%\begin{figure}[t]
%\begin{center}
%\psfig{file=Bsimilar.eps,width=300pt}
%\end{center}
%\caption{NKY algorithm: performance of different implementations.}
%\label{fig:Bsimilar}
%\end{figure}
%

% CITING
% \citep{citation}

\begin{document}

% uncomment this to get journal footers
%\ecjHeader{x}{x}{xxx-xxx}{200X}{genetic vs deterministic algorithms for LCS}{C. A. Andreoni, U. Masood}
\title{Caching Fractal B+ Trees}

\author{\name{\bf Claudio Alberto Andreoni} \hfill \addr{caa@mit.edu}\\ 
        \addr{Department of Mathematics, Massachusetts Institute of Technology, 
        Cambridge, 02139, United States}
\AND
       \name{\bf Usman Masood} \hfill \addr{usmanm@mit.edu}\\
        \addr{Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 
        Cambridge, 02139, United States}
}

\maketitle

\begin{abstract}
The two primary requirements for database systems are handling data queries rapidly and utilizing storage thriftily.
However, these often stand in contrast with each other,
as traditionally space usage and achievable performance are inversely proportional.
In order to quickly retrieve data,
databases maintain an index of searchable keys, along with the location of the corresponding data on drive.
Modern systems such as IBM DB2, Oracle, and MS SQL Server use B+ trees for the indexes,
due to their ability to efficiently exploit disk bandwidth \citep{Gehrke:2002}.

Fractal B+ trees (FB+ trees), a kind of recursive B+ tree structure, were proposed to improve performance
by adapting to both disk layout and processor cache.
However, these either increase the amount of space wasted in representing the index,
or require an aggressive memory management scheme \citep{Chen:2002}.

We propose a new data structure named Caching Fractal B+ trees (CFB+ trees), which replicates the structure
of FB+ trees, but takes advantage of the wasted space to perform online caching of database entries.
CFB+ trees simplify the complexity associated with performing index operations,
and reduce in expectation the number of reads from disk necessary to retrieve the queried data. 
\end{abstract}

\section{Introduction}

One of the key aspects of data design is that resources, especially disk bandwidth and memory, must be utilized efficiently. To utilize memory efficiently, databases cache \textit{hot} tuples in memory so that when querying for them, they can be fetched directly from memory rather than having to go to the disk. To utilize disk bandwidth efficiently, databases try to minimze the amount of disk pages read and perform sequential access to disk when possible. Disks are usually an order of magnitude slower than memory, and since data in databases needs to be persistent stored on disk, disk bandwidth tends to be the bottleneck for practically all workload.

To that end, databases often employ index structures over certain \textit{columns} to store tuple pointers so that they can be looked up in a way faster than scanning the entire relation. As long as the cost of reading the entire relation is less than the cost of searching for an item in the index structure and fetching a single tuple from disk, we should expect a performance improvement. In practice, all relations are large enough that index structures are helpful rather than detrimental to performance.

The way these index structures are used is that while adding a tuple we insert an entry to the index structure with the \textit{key} as the contents of the column we are indexing over, and the \textit{value} as the pointer to the on-disk location of the tuple. When retrieving a tuple, we search for the tuple in the index structure and use the pointer returned by it to read the tuple from disk. 

\subsection{B+ tree Indexes}
The index structure should provide operations such as insertion, deletion, search and in most cases range scan. B+ trees are a natural choice for index structures because they provide all the required operations and they can be optimized to exploit disk bandwidth efficiently. Some times hash tables are also used to index data, but since they do not support range queries they can only be used to index columns over which range queries are never performed. Lately, cache-oblivious Fractal Trees have also been employed as index structures.


\subsubsection{Optimizing B+ trees for Disk Performance}

To optimize I/O performance, traditional \textit{disk optimized} B+ trees are composed of nodes the size equal to a \textit{disk page} -- the natural transfer size for reading and writing to disk. Having such large nodes, means that the branching factor is usually quite large, on the order of hundreds. Having such a large branching factor means that the height of the tree is quite small. This is beneficial because only a few pages need to be read from disk inorder to find the tuple pointer we're looking for. At the same time, however, this also means that the additional page access to retrieve the associated tuple data is a significant part of the total cost. Typical index fill factors have been shown to be around 68\%, which is a concious decision to ensure that in the presense of inserts we do not have too many expensive node split operations. A major contribution of this paper is providing a scheme to utilize this wasted space as a tuple cache, so that for \textit{hot} tuples we can read the data directly from the leaf node page and avoid the additional disk page access we talked about earlier needed to reading tuple data.

A downside of using traditional \textit{disk optimized} B+ trees is that they have poor cache performance. They incur an excessive number of cache misses, wasting time and forcing the eviction of useful data from the cache in the process. This is because of the large the discrepancy in node sizes and width of cache lines -- the natural size of reading and writing to main memory. A single cache line is usually 32B-128B wide and so the \textit{jumps} in a 4KB address space while binary searching a node are result in a lot of cache misses. Cache prefetching is helpful only when the range of the binary search becomes considerably small.

\subsubsection{Optimizing B+ trees for Cache Performance}

Recently, many studies have presented new types of B+ trees -- \textit{cache-sensitive B+ trees}, \textit{prefetching B+ trees} -- optimized for cache performance. These datastructures try to minimize the impact of cache misses by making node sizes equal to to the width of a cache line (or some small multiple of it). However, this technique for cache optimization is at odds with disk performance. Small nodes result in a small branching factor, which means in searching from the root to the desired leaf, we might suffer a disk page access for each node on this path.

Fractal Prefetching B+ trees (fpB+ trees) are a type of B+ tree which try to optimize both disk and cache performance. In this paper, we propose Caching Fractal B+ trees (cfB+ trees) which extend fpB+ trees to improve cache performance further while ensuring comparable disk performance.

\section{Fractal B+ trees}
\usman
\begin{enumerate}
	\item Explain the data structure (don't go into SPLIT and MERGE though, I need to talk about these later)
	\item Say that the way they are used is index adn then you have to access the disk separately later 
	\item Give rationale for usage: have best disk performance, want to improve cache performance
	\item Add all your interesting observations about the relationship between parameters (bfactor etc etc)
\end{enumerate}
\claudio
Will make graphs if necessary to explain something

\section{Adding Caching: CFB+ trees}
\claudio
\subsection{Using Gaps for Good}
This far, we have illustrated how FB+ trees store a small B+ tree in each big node.
The strategy proposed for FB+ trees is to fragment big nodes into smaller nodes when the small trees
are mostly empty, and pack multiple big nodes in the same disk page.
Instead, when big nodes grow, the inverse strategy is applied and big nodes are split and spread
across pages.

This process adds complexity to the implementation, and only guarantees that pages are filled
homogeneously, not that space waste is minimized.
Indeed, when a big block the size of a page is split in half,
we can expect the former page to become half empty and the new one to become half full.

We propose instead that a page always represent a single big node, and the space gaps in the page
be exploited to cache database tuples.
Compared to the strategy for FB+ trees, we avoid compressing big nodes, resulting in an
even less efficient tree space usage.
However, we also exploit all the space which is not allocated to the tree to store cached tuples,
so that almost no space is left unused.

This allows to return searched tuples which are in cache without the additional overhead of reading a page from
the actual tuple storage.
The operating system tries to maintain pages which are accessed frequently in memory for faster
access.
Also, pages belonging to the index are accessed more often than tuple pages, because every operation
needs to walk a path in the index.
Therefore, caching tuples in the index has the double advantage of reducing the likelihood
of a read to tuple page which is not in memory, and reducing the need for the operating systems
to replace index pages in memory with tuple pages.
The same fact remains relevant for integrated database systems, which strongly enforce the policy of keeping
index pages in memory.


\subsection{Patterns of Memory (Un)Usage}

We refer to the memory space necessary to store a small node as a \textit{slot}.
Each big node contains a fixed number $\sigma$ of slots, part of which will be in use
as small nodes, while the rest will be available for cache (Figure \ref{fig:inner_block}).
\begin{figure}[h]
\begin{center}
\includegraphics[width=350pt]{inner_block}
\end{center}
\caption{
The content of a big node.
On the left, a small tree as it is interpreted logically.
On the right, a small tree laid out in memory, with slots available for cache.
}
\label{fig:inner_block}
\end{figure}

In order to justify our choice to utilize unused slots as cache,
we recall the fact provided earlier that
B+ trees tend to be full at around 68\% of their capacity under standard database loads \citep{Wu:2011}.
Thus, given that the tree should be fairly well balanced,
we expect to have $0.3 \sigma$ slots available for cache on average in leaf big blocks.
As long as tuples are small enough to fit in a single slot,
we expect then to be able to cache around half of the tuples indexed in the tree.
Otherwise, we can store a projection of the data, meaning a subset of the most utilized fields for each entry,
and still read the disk storage if we don't find a field we are interested in.

Furthermore, unused slots are all the same size, and they are all located at an integer multiple
of the slot size, making them easy to access and fast to read without much overhead.


\subsection{Caching tuples}
An important fact to notice is that the pattern of insertions and removals determines the shape of the tree;
therefore, the distribution of slots available for cache in memory is hard to analyze.
Moreover, as items are inserted and removed, new nodes are also added and removed;
correspondingly, slots that were once available are allocated to the
tree and become unavailable to the cache, and vice-versa.
This is evident even from Figure \ref{fig:inner_block}, which illustrates that
slots available for cache are sparse in the big node.

It is apparent then that the scenario is reminescent of that of a hash table.
In fact, we can consider the function that allocates slots to the small tree to be an adversary
hash function taking away and releasing cache slots outside of the control of our cache system.
We can then interpret the problem as that of finding a suitable hash system to insert and retrieve
cache entries in unused slots.

Intuitively, a candidate hashing strategy can take inspiration from open addressing.
Conside a big node $B$ on the path to the leaf that points to tuple $\tau$.
With open addressing, we insert by repeatedly hashing the key for $\tau$ to a slot in $B$,
till we find a slot that is available for cache and has room for one more cache entry.
Similarly, we search by repeatedly hashing the key for $\tau$ till we either find
a cache slot containing $\tau$, we find a cache slot with room for more entries,
or we have probed every slot in the big node.
With this scheme, a cache slot converted to a tree node effectively results
in the cache entries at that slot being evicted from cache.

Notice that, unlike for the case of traditional open addressing, we do not assume an item is not cached
if we hash to a slot that has available room but does not contain it.
That is because otherwise, when a tree node is released and made available from cache, we would risk
invalidating parts of the cache.
In spite of the fact that we may need to probe all slots often, performance should not degrade in practice
thanks to prefetching.
If we employ linear probing, the processor will fetch the next slots as we process the current one,
resulting in an almost negligible cost to access the entire big node.
That is especially the case for modern processors, which have data L1 cache in the order of tens of KB,
so the entire big node can be prefected to near processor cache.

Inconveniently, the efficiency of open addressing with linear probing decays very rapidly as the fill factor
of the big node increases, canceling the advantages of hashing compared to linearly scanning all slots.
Also, strategies that are more theoretically appealing such as cuckoo and hopscotch hashing
would not fit the problem conveniently, as displacing a slot used as a node would require to update all references
to it from other nodes.

A possible improvement could come from implementing a coalesced hashing strategy \citep{Vitter:1987}.
Coalesced hashing unites chaining with open addressing by storing chains of items that would
share the same bucket sparsely throughout the table but connected by pointers.
This reduces clustering, while maintaining the ability to use small, fixed-sized slots as opposed to growing chains.
In the worst case with a long, sparse chain the behavior of coalesced hashing would approximated a linear
scan in random order, voiding the benefits of prefetching.
Therefore, we leave the problem of improving over open addressing for a future implementation.

Another problem that we leave open to further investigation is eviction order when the cache is full.
As we previously stated, some evictions will necessarily happen when a cache slot is upgraded to a node slot.
However, in many typical database access scenarios, searches largely dominate insertions and deletions.
Therefore, we would like to have an eviction policy for the case of long series of queries, for instance LRU.
The main difficulty of implementing a least-recently accessed logic would be the reduction
in available cache space due to the need to store additional information for access time,
or the need to frequently rearrange cached items to maintain them ordered by access time.
Given that slots are already of comparably or lesser sized than an average tuple, it would be a challenge
to add metainformation, but it is conceivable that a smart rearrangement process could be fruitful.

\section{Experimental Results}

\subsection{Implementations}
\claudio
For all benchmarks in this paper, we use a system based on an Intel Core i7 920 with 4 GB of RAM.
All code was written in the C language, and compiled with gcc version 4.6.1 with level-3 optimizations enabled.

We produced three implementations: a basic, unoptimized B+ tree as control, an FB+ tree and a CFB+ tree.
The FB+ tree follows the same design guidelines that we laid down in this paper for CFB+ trees,
so that the two differ solely by the presence or absence of our data caching strategy.
This delegates the task of maintaining the first few levels of the tree in memory to the system page cache,
a choice that can be easily reversed by adding a memory management framework.

Slots in big nodes are allocated/deallocated to/from the tree with an elementary linear probing strategy,
and small nodes are also searched linearly.
Processor cache is instrumental in making these operations efficient independently from their asymptotic performance.

Big nodes are managed lazily, so that insertions are appended after the last one, and removals results in gaps.
When the number of gaps increases to equal the number of used blocks, we compact the entire structure.
This yields an amortized constant cost, because only one block is moved for every block that was deleted.
Also, it ensure that the total space occupied by the structure is always asymptotically linear in the number
of big nodes being actively used.

We support only the basic query operations, leaving the more complex range queries to a future improvement.
We notice however that, just like FB+ trees, CFB+ trees can be augmented with jump pointers to that end.
All the leaves with values would be stored outside the tree in a sorted, contiguous fashion.
Jump pointers would be maintained in the tree, and a search to the leftmost element in a range would be performed.
Then, it would be sufficient to scan the leaves rightwards till the right boundary of the range is exceeded.
Again, due to the contiguous nature of the leaves, prefetching would help the performance of the operation.

A problem that could be helped by future work is reducing the implementation complexity for the CFB+ tree.
The two different granularities at which the structure exist, big tree with big nodes and small trees with small nodes
at each big node, results in a high complexity burden on the developer.
The most complex operation is splitting, as it recurses at both granularities, intertwining insertions in parents
and copies of children with the need to keep all relative likes between nodes updated throughout the move.
Thus, it would benefit from a cleaner interface between the operations than the one we designed under
time constraints. 
This complexity connected with keeping ancestors-descendants coherency at different granularities
is also what prompted us not to add additional granularities to exploit the three levels of processor cache
or to adapt the structure for cache-obliviousness.


\subsection{Benchmarks}
\usman
\begin{enumerate}
	\item Make benchmarks and collect all performance data
	\item Explain why the benchmarks are relevant / useful / interesting
\end{enumerate}
\claudio
Make plots after the data is collected


\section{Conclusions}
\claudio
Summarize and remark that we managed to improve.

\small

\bibliographystyle{apalike}
\bibliography{final}


\end{document}
